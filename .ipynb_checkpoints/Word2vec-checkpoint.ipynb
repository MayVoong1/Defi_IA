{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import gensim\n",
    "import sklearn.model_selection as sms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"data/train.json\").set_index('Id')\n",
    "test_df = pd.read_json(\"data/test.json\").set_index('Id')\n",
    "train_label = pd.read_csv(\"data/train_label.csv\").set_index('Id')\n",
    "categories_df= pd.read_csv(\"data/categories_string.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.concat([train_df,train_label],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean import CleanText\n",
    "ct = CleanText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.clean_df_column(data_train, \"description\",\"description_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.clean_df_column(test_df, \"description\",\"description_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Vectorization: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dimension = 300\n",
    "min_count = 1\n",
    "window = 5\n",
    "hs = 0\n",
    "negative = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_token = [line.split(\" \") for line in data_train[\"description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in test_df[\"description_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding import WordEmbedding\n",
    "\n",
    "we_sg = WordEmbedding(word_embedding_type = \"word2vec\", \n",
    "                      args = dict(sentences = array_token, sg=1, hs=hs, negative=negative, min_count=min_count, size=features_dimension, window = window, iter=10))\n",
    "model_sg, training_time_sg = we_sg.train()\n",
    "print(\"Model Skip-gram trained in %.2f minutes\"%(training_time_sg/60))\n",
    "model_sg.save(\"model_sg_100k\")\n",
    "\n",
    "we_cbow = WordEmbedding(word_embedding_type = \"word2vec\", \n",
    "                      args = dict(sentences = array_token, sg=0, hs=hs, negative=negative, min_count=min_count, size=features_dimension, window = window, iter=10))\n",
    "model_cbow, training_time_cbow = we_cbow.train()\n",
    "print(\"Model CBOW trained in %.2f minutes\"%(training_time_cbow/60))\n",
    "model_cbow.save(\"model_cbow_100k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain, datavalid = sms.train_test_split(data_train, test_size=0.1, random_state=42)\n",
    "train_array_token = [line.split(\" \") for line in datatrain[\"description_cleaned\"].values]\n",
    "valid_array_token = [line.split(\" \") for line in datavalid[\"description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in test_df[\"description_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_train_cbow, embedded_conversion_train_time_cbow = WordEmbedding.get_matrix_features_means(array_token, model_cbow)\n",
    "X_embedded_valid_cbow, embedded_conversion_valid_time_cbow = WordEmbedding.get_matrix_features_means(valid_array_token, model_cbow)\n",
    "X_embedded_test_cbow, embedded_conversion_test_time_cbow = WordEmbedding.get_matrix_features_means(test_array_token, model_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_train_sg, embedded_conversion_train_time_sg = WordEmbedding.get_matrix_features_means(array_token, model_sg)\n",
    "X_embedded_valid_sg, embedded_conversion_valid_time_sg = WordEmbedding.get_matrix_features_means(valid_array_token, model_sg)\n",
    "X_embedded_test_sg, embedded_conversion_test_time_sg = WordEmbedding.get_matrix_features_means(test_array_token, model_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_embedded_train_sg\n",
    "Y_train=datatrain.Category.values\n",
    "X_valid = X_embedded_valid_sg\n",
    "Y_valid=datavalid.Category.values\n",
    "X_test =X_embedded_test_sg\n",
    "\n",
    "X_train_cbow = X_embedded_train_cbow\n",
    "Y_train_cbow = data_train.Category.values\n",
    "X_valid_cbow = X_embedded_valid_cbow\n",
    "Y_valid_cbow = datavalid.Category.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"C\":[0.1, 1, 5,10]}\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10,scoring=\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "logreg_cv.fit(X_train,Y_train)\n",
    "te=time.time()\n",
    "temps=te-ts\n",
    "print(\"Time =\", temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg_cv.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_valid,predictions,average=\"macro\")\n",
    "print(logreg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_gs_w2v_skipgram.pkl', 'wb') as fid:\n",
    "    pickle.dump(logreg_cv, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_gs_w2v_skipgram_results.pkl', 'wb') as fid:\n",
    "    pickle.dump(logreg_cv.cv_results_, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cbow method\n",
    "grid = {\"C\":[0.1, 1, 5,10]}\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "logreg_cv.fit(X_train_cbow,Y_train_cbow)\n",
    "te=time.time()\n",
    "temps=te-ts\n",
    "print(\"Time =\", temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg_cv.predict(X_valid_cbow)\n",
    "f1_score(Y_valid_cbow,predictions,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_gs_w2v_cbow.pkl', 'wb') as fid:\n",
    "    pickle.dump(logreg_cv, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_gs_w2v_cbow_results.pkl', 'wb') as fid:\n",
    "    pickle.dump(logreg_cv.cv_results_, fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\"hidden_layer_sizes\" : [128, 256]}\n",
    "mlp_grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "mlp_grid_search =mlp_grid_search.fit(X_train,Y_train)\n",
    "print(mlp_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_valid=mlp_grid_search.predict(X_valid,Y_valid)\n",
    "f1_score(Y_valid_cbow,mlp_valid,average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500, 800]\n",
    "}\n",
    "\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "rf_grid_search = GridSearchCV(rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2,scoring=\"f1_macro\")\n",
    "\n",
    "ts = time.time()\n",
    "rf_grid_search.fit(X_train,Y_train)\n",
    "te=time.time()\n",
    "temps=te-ts\n",
    "print(\"Time =\", temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_gs_w2v_skipgram.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf_grid_search, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prediction=rf.predict(X_valid,Y_valid)\n",
    "f1_score(Y_valid,rf_prediction,average=\"macro\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Web_Scraping_inca.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:TEST]",
   "language": "python",
   "name": "conda-env-TEST-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
